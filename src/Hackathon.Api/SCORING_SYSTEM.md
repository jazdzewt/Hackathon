# System Oceniania HackathonÃ³w - Dokumentacja

## ğŸ“‹ PrzeglÄ…d

System obsÅ‚uguje dwa tryby oceniania submisji uczestnikÃ³w:
1. **Automatyczne** - porÃ³wnanie z plikiem ground-truth
2. **RÄ™czne** - ocena przez admina/sÄ™dziego

---

## ğŸ—ï¸ Architektura

### Modele Danych

#### `ChallengeAsset` (Nowy)
Przechowuje zasoby wyzwania, w tym ukryte pliki ground-truth:
```csharp
- Id: string (GUID)
- ChallengeId: string
- AssetType: string ("ground_truth", "dataset", "sample_submission")
- FileName: string
- FileUrl: string (URL w Supabase Storage)
- FileSizeMb: decimal?
- IsPublic: bool (false dla ground-truth!)
- UploadedBy: string (ID admina)
- CreatedAt: DateTime
```

#### `Submission` (Rozszerzony)
Dodane pola do Å›ledzenia oceniania:
```csharp
- EvaluationStatus: string ("pending", "auto_evaluated", "manually_evaluated")
- EvaluatorId: string? (ID admina/sÄ™dziego dla rÄ™cznej oceny)
- EvaluatorNotes: string? (notatki sÄ™dziego)
- EvaluatedAt: DateTime?
```

### Serwisy

#### `IScoringService` / `ScoringService`
GÅ‚Ã³wny serwis odpowiedzialny za ocenianie:

**Metody:**
- `EvaluateSubmissionAsync(string submissionId)` - automatyczna ocena
- `ManuallyScoreSubmissionAsync(string submissionId, decimal score, string? notes, string evaluatorId)` - rÄ™czna ocena
- `CalculateScoreAsync(byte[] submissionFile, byte[] groundTruthFile, string evaluationMetric, string fileExtension)` - obliczenie wyniku

**Wspierane metryki:**
- `accuracy` - dokÅ‚adnoÅ›Ä‡ klasyfikacji
- `f1` / `f1-score` - F1-score dla klasyfikacji binarnej
- `mse` / `mean-squared-error` - bÅ‚Ä…d Å›redniokwadratowy (regresja)
- `mae` / `mean-absolute-error` - bÅ‚Ä…d Å›redni absolutny (regresja)
- `rmse` / `root-mean-squared-error` - pierwiastek bÅ‚Ä™du Å›redniokwadratowego

**Wspierane formaty plikÃ³w:**
- `.csv` - porÃ³wnanie wartoÅ›ci w ostatniej kolumnie
- `.json` - wymaga formatu `{ "predictions": [...] }`
- `.txt` - porÃ³wnanie linijka po linijce

#### `SubmissionService` (Zaktualizowany)
Integracja z `ScoringService`:
- Po upload submission automatycznie uruchamia ocenianie w tle (asynchronicznie)
- Oblicza hash pliku (SHA256) dla wykrywania duplikatÃ³w
- Waliduje rozmiar i typ pliku zgodnie z reguÅ‚ami challenge

---

## ğŸ” BezpieczeÅ„stwo Ground-Truth

### Storage Structure
```
datasets/
â”œâ”€â”€ challenges/              # Datasety publiczne
â”‚   â””â”€â”€ {challengeId}.csv
â””â”€â”€ ground-truth/            # Pliki ground-truth (PRYWATNE!)
    â””â”€â”€ {challengeId}.csv
```

### Polityka dostÄ™pu:
- âœ… **Admin** - peÅ‚ny dostÄ™p (upload, download, delete)
- âœ… **Serwis oceniania** - read-only dostÄ™p przez Service Role Key
- âŒ **Uczestnicy** - BRAK DOSTÄ˜PU (RLS policy)

### Row Level Security (RLS)
W tabeli `challenge_assets`:
```sql
-- Uczestnicy NIE widzÄ… ground-truth
CREATE POLICY "Participants cannot see ground truth"
ON challenge_assets
FOR SELECT
USING (
  is_public = true 
  OR auth.uid() IN (
    SELECT user_id FROM profiles WHERE role = 'admin'
  )
);
```

---

## ğŸš€ Endpointy API

### 1. Submit Solution (Uczestnik)
```http
POST /api/submissions/challenges/{challengeId}/submit
Authorization: Bearer {JWT_TOKEN}
Content-Type: multipart/form-data

Body:
- file: IFormFile
```

**Flow:**
1. Walidacja challenge (czy aktywny, czy nie minÄ…Å‚ deadline)
2. Walidacja pliku (rozmiar, typ)
3. Obliczenie SHA256 hash (wykrywanie duplikatÃ³w)
4. Upload do Storage: `datasets/submissions/{userId}/{challengeId}/{submissionId}.csv`
5. Zapis do bazy z statusem `pending`
6. **Asynchroniczne** uruchomienie oceniania w tle

**Response:**
```json
{
  "message": "Submission accepted and will be evaluated shortly",
  "submissionId": "uuid"
}
```

### 2. Upload Ground-Truth (Admin Only)
```http
POST /api/admin/challenges/{id}/ground-truth
Authorization: Bearer {ADMIN_JWT_TOKEN}
Content-Type: multipart/form-data

Body:
- file: IFormFile
```

**Flow:**
1. Upload pliku do `datasets/ground-truth/{challengeId}.csv`
2. Utworzenie rekordu w `challenge_assets`:
   - `asset_type = "ground_truth"`
   - `is_public = false` âš ï¸
3. Aktualizacja `challenges.ground_truth_url`

**Response:**
```json
{
  "message": "Ground truth file uploaded successfully (hidden from participants)",
  "assetId": "uuid"
}
```

### 3. Manual Score (Admin/Judge Only)
```http
POST /api/admin/submissions/{submissionId}/score
Authorization: Bearer {ADMIN_JWT_TOKEN}
Content-Type: application/json

Body:
{
  "score": 95.5,
  "notes": "Bardzo kreatywne podejÅ›cie, ale maÅ‚y bÅ‚Ä…d w edge case."
}
```

**Flow:**
1. Aktualizacja `submissions`:
   - `score = 95.5`
   - `evaluation_status = "manually_evaluated"`
   - `evaluator_id = {admin_user_id}`
   - `evaluator_notes = "..."`
   - `evaluated_at = DateTime.UtcNow`

**Response:**
```json
{
  "message": "Submission scored successfully",
  "score": 95.5,
  "evaluatedBy": "admin_user_id"
}
```

### 4. Re-evaluate Submission (Admin Only)
```http
POST /api/admin/submissions/{submissionId}/reevaluate
Authorization: Bearer {ADMIN_JWT_TOKEN}
```

Wymusza ponowne automatyczne przeliczenie wyniku (np. po zmianie ground-truth).

---

## ğŸ”„ Proces Automatycznego Oceniania

### 1. Trigger
```csharp
// W SubmissionService.SubmitSolutionAsync()
_ = Task.Run(async () =>
{
    await EvaluateSubmissionAsync(submissionId);
});
```

### 2. Evaluation Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Pobierz submission z bazy        â”‚
â”‚    Status = "processing"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Pobierz challenge + ground-truth â”‚
â”‚    asset (WHERE asset_type =        â”‚
â”‚    "ground_truth")                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Download plikÃ³w z Storage:       â”‚
â”‚    - submission.file_url            â”‚
â”‚    - ground_truth.file_url          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Oblicz SHA256 hash submission    â”‚
â”‚    (deterministycznoÅ›Ä‡)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. CalculateScoreAsync()            â”‚
â”‚    - Parsuj pliki (CSV/JSON/TXT)    â”‚
â”‚    - Zastosuj metrykÄ™ ewaluacji     â”‚
â”‚    - ZwrÃ³Ä‡ score (0-100)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Zapisz wynik do bazy:            â”‚
â”‚    - score = {calculated}           â”‚
â”‚    - status = "completed"           â”‚
â”‚    - evaluation_status =            â”‚
â”‚      "auto_evaluated"               â”‚
â”‚    - evaluated_at = Now             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Error Handling
JeÅ›li ocenianie siÄ™ nie powiedzie:
```csharp
submission.Status = "failed";
submission.ErrorMessage = ex.Message;
```

---

## ğŸ“Š PrzykÅ‚ady UÅ¼ycia

### PrzykÅ‚ad 1: Accuracy dla Klasyfikacji (CSV)

**Ground-Truth (`ground-truth/challenge-123.csv`):**
```csv
id,prediction
1,cat
2,dog
3,cat
4,bird
```

**User Submission:**
```csv
id,prediction
1,cat
2,cat
3,cat
4,bird
```

**Wynik:** 75% accuracy (3/4 poprawne)

---

### PrzykÅ‚ad 2: MSE dla Regresji (JSON)

**Ground-Truth:**
```json
{
  "predictions": [1.0, 2.5, 3.2, 4.8]
}
```

**User Submission:**
```json
{
  "predictions": [1.1, 2.4, 3.3, 4.7]
}
```

**Obliczenie:**
```
MSE = [(1.1-1.0)Â² + (2.4-2.5)Â² + (3.3-3.2)Â² + (4.7-4.8)Â²] / 4
    = [0.01 + 0.01 + 0.01 + 0.01] / 4 = 0.01
Score = 100 * exp(-0.01) â‰ˆ 99.00
```

---

## ğŸ”§ Konfiguracja

### Program.cs
```csharp
builder.Services.AddScoped<IScoringService, ScoringService>();
builder.Services.AddScoped<ISubmissionService, SubmissionService>();
```

### appsettings.json
```json
{
  "Supabase": {
    "Url": "https://your-project.supabase.co",
    "ServiceRoleKey": "eyJhbG...", // âš ï¸ Wymagane dla dostÄ™pu do ground-truth
    "AnonKey": "eyJhbG..."
  }
}
```

---

## âœ… Checklist Implementacji

### Backend (API)
- [x] Model `ChallengeAsset` utworzony
- [x] Model `Submission` rozszerzony (evaluation_status, evaluator_id, evaluator_notes, evaluated_at)
- [x] `IScoringService` + implementacja
- [x] `SubmissionService` zintegrowany z `ScoringService`
- [x] Endpoint: `POST /api/submissions/challenges/{id}/submit`
- [x] Endpoint: `POST /api/admin/challenges/{id}/ground-truth`
- [x] Endpoint: `POST /api/admin/submissions/{id}/score`
- [x] Endpoint: `POST /api/admin/submissions/{id}/reevaluate`
- [x] Asynchroniczne ocenianie w tle
- [x] Obliczanie SHA256 hash dla submisji
- [x] Wsparcie dla CSV, JSON, TXT
- [x] Metryki: accuracy, F1, MSE, MAE, RMSE

### Database (Supabase)
- [ ] Tabela `challenge_assets` utworzona
- [ ] Kolumny w `submissions` dodane:
  - [ ] `evaluation_status`
  - [ ] `evaluator_id`
  - [ ] `evaluator_notes`
  - [ ] `evaluated_at`
- [ ] RLS policy dla `challenge_assets` (is_public = false dla ground-truth)
- [ ] Bucket `datasets` z folderem `ground-truth/`
- [ ] Storage policy: tylko admin i service role majÄ… dostÄ™p do `ground-truth/`

### Frontend (TODO)
- [ ] Interfejs uploadu ground-truth dla admina
- [ ] Panel rÄ™cznego oceniania submisji
- [ ] PodglÄ…d statusu oceniania (pending/processing/completed)
- [ ] WyÅ›wietlanie notatek sÄ™dziego dla uczestnikÃ³w

---

## ğŸ¯ PrzykÅ‚adowe Scenariusze

### Scenariusz 1: Challenge z Automatycznym Ocenianiem
1. Admin tworzy challenge z metrykÄ… `accuracy`
2. Admin uploaduje `ground-truth.csv` przez endpoint
3. Uczestnik uploaduje swojÄ… submisjÄ™
4. System automatycznie (w tle) ocenia i zapisuje wynik
5. Wynik pojawia siÄ™ na leaderboardzie

### Scenariusz 2: Challenge z RÄ™cznym Ocenianiem
1. Admin tworzy challenge
2. Uczestnik uploaduje submisjÄ™ (np. prezentacjÄ™ wideo)
3. Submisja ma status `pending`
4. SÄ™dzia loguje siÄ™ i ocenia przez panel admina
5. Wpisuje score (0-100) i notatkÄ™
6. Wynik pojawia siÄ™ na leaderboardzie

### Scenariusz 3: Hybrydowe Ocenianie
1. System automatycznie ocenia wszystkie submisje
2. Admin przeglÄ…da top 10 submisji
3. Admin rÄ™cznie koryguje wyniki podejrzanych submisji
4. `evaluation_status` zmienia siÄ™ na `manually_evaluated`

---

## ğŸ“ Notatki Techniczne

### Dlaczego SHA256 hash?
- Wykrywanie duplikatÃ³w submisji
- Deterministyczna identyfikacja pliku
- Zabezpieczenie przed wielokrotnym uploadem tego samego rozwiÄ…zania

### Dlaczego asynchroniczne ocenianie?
- Nie blokuje response do uÅ¼ytkownika
- MoÅ¼na oceniaÄ‡ ciÄ™Å¼kie submisje (duÅ¼e pliki, skomplikowane metryki)
- Lepsze UX - uÅ¼ytkownik dostaje natychmiastowe potwierdzenie

### Dlaczego score 0-100?
- Standaryzacja - Å‚atwe porÃ³wnanie rÃ³Å¼nych metryk
- Przyjazne dla uÅ¼ytkownikÃ³w (procenty)
- Dla MSE/MAE uÅ¼ywamy `score = 100 * exp(-error)` aby przeksztaÅ‚ciÄ‡ na skalÄ™ 0-100

---

## ğŸš¨ Potencjalne Problemy i RozwiÄ…zania

### Problem: Ground-truth jest zbyt duÅ¼y
**RozwiÄ…zanie:** Kompresja (zip) lub sampling (wzorce losowe)

### Problem: Ocenianie trwa zbyt dÅ‚ugo
**RozwiÄ…zanie:** 
- Queue system (RabbitMQ/Redis)
- Timeout dla oceniania
- Partial scoring dla duÅ¼ych datasetÃ³w

### Problem: UÅ¼ytkownik uploaduje zÅ‚oÅ›liwy plik
**RozwiÄ…zanie:**
- Skanowanie antywirusowe przed ocenÄ…
- Sandbox dla execution (jeÅ›li potrzebne)
- Limit rozmiaru pliku

### Problem: Cheat detection
**RozwiÄ…zanie:**
- Analiza podobieÅ„stwa submisji (plagiat detection)
- Rate limiting na submissions
- Manual review dla podejrzanych wynikÃ³w

---

## ğŸ“š Dalszy RozwÃ³j

### Faza 2 (PrzyszÅ‚oÅ›Ä‡)
- [ ] Wsparcie dla wÅ‚asnych metryk (custom scoring functions)
- [ ] Partial scoring (wyniki per test case)
- [ ] Leaderboard freezing (zamroÅ¼enie przed koÅ„cem)
- [ ] Public/Private test splits
- [ ] Submission history i porÃ³wnanie wynikÃ³w
- [ ] Automated cheating detection
- [ ] Real-time leaderboard updates (WebSockets)

---

## ğŸ‘¨â€ğŸ’» Autorzy
System oceniania zaimplementowany zgodnie z wymaganiami platformy hackathonowej.

Data implementacji: 15.11.2025
